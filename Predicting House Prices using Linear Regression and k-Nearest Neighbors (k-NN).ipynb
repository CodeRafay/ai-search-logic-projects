{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9cb7a9be",
      "metadata": {
        "id": "9cb7a9be"
      },
      "source": [
        "## **Predicting House Prices using Linear Regression and k-Nearest Neighbors (k-NN)**\n",
        "\n",
        "### **Task Overview**\n",
        "\n",
        "The goal of this project was to predict the **median house value** in California districts using the California Housing dataset. We aimed to compare two regression models:  \n",
        "- **Linear Regression** (parametric)  \n",
        "- **k-NN Regression** (nonparametric, k = 5)\n",
        "\n",
        "We evaluated model performance using standard metrics like **R² (coefficient of determination)**, **Mean Absolute Error (MAE)**, and **Mean Squared Error (MSE)**.\n",
        "\n",
        "### **Dataset Description**\n",
        "\n",
        "We used the California Housing dataset from scikit-learn, which contains features like:\n",
        "- Median income\n",
        "- Average house age\n",
        "- Average rooms\n",
        "- Population\n",
        "- Latitude, Longitude, etc.\n",
        "\n",
        "The **target variable** is:\n",
        "- `MedHouseVal` → Median house value for each block group (in $100,000s)\n",
        "\n",
        "### **Key Takeaways**\n",
        "\n",
        "- **Linear Regression** is a parametric model that assumes a linear relationship between features and the target.\n",
        "- **k-NN Regression** is a nonparametric model that makes predictions based on the average of the k nearest neighbors.\n",
        "- We used multiple metrics to get a complete picture of model performance.\n",
        "- Standardization was essential to ensure fair distance calculations for the k-NN model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d5773f68",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading California Housing Dataset...\n",
            "Dataset shape: (20640, 8)\n",
            "\n",
            "Feature names:\n",
            "- MedInc\n",
            "- HouseAge\n",
            "- AveRooms\n",
            "- AveBedrms\n",
            "- Population\n",
            "- AveOccup\n",
            "- Latitude\n",
            "- Longitude\n",
            "\n",
            "Target variable: MedHouseVal\n",
            "\n",
            "Basic statistics of features:\n",
            "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
            "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   \n",
            "mean       3.870671     28.639486      5.429000      1.096675   1425.476744   \n",
            "std        1.899822     12.585558      2.474173      0.473911   1132.462122   \n",
            "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
            "25%        2.563400     18.000000      4.440716      1.006079    787.000000   \n",
            "50%        3.534800     29.000000      5.229129      1.048780   1166.000000   \n",
            "75%        4.743250     37.000000      6.052381      1.099526   1725.000000   \n",
            "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
            "\n",
            "           AveOccup      Latitude     Longitude  \n",
            "count  20640.000000  20640.000000  20640.000000  \n",
            "mean       3.070655     35.631861   -119.569704  \n",
            "std       10.386050      2.135952      2.003532  \n",
            "min        0.692308     32.540000   -124.350000  \n",
            "25%        2.429741     33.930000   -121.800000  \n",
            "50%        2.818116     34.260000   -118.490000  \n",
            "75%        3.282261     37.710000   -118.010000  \n",
            "max     1243.333333     41.950000   -114.310000  \n",
            "\n",
            "Training set shape: (16512, 8)\n",
            "Testing set shape: (4128, 8)\n",
            "\n",
            "Training Linear Regression model...\n",
            "\n",
            "Linear Regression Performance:\n",
            "R² Score: 0.5758\n",
            "Mean Absolute Error: 0.5332\n",
            "Mean Squared Error: 0.5559\n",
            "Root Mean Squared Error: 0.7456\n",
            "\n",
            "Training k-NN Regression model with k=3...\n",
            "\n",
            "k-NN (k=3) Performance:\n",
            "R² Score: 0.6439\n",
            "Mean Absolute Error: 0.4599\n",
            "Mean Squared Error: 0.4667\n",
            "Root Mean Squared Error: 0.6831\n",
            "\n",
            "Training k-NN Regression model with k=5...\n",
            "\n",
            "k-NN (k=5) Performance:\n",
            "R² Score: 0.6700\n",
            "Mean Absolute Error: 0.4462\n",
            "Mean Squared Error: 0.4324\n",
            "Root Mean Squared Error: 0.6576\n",
            "\n",
            "Training k-NN Regression model with k=7...\n",
            "\n",
            "k-NN (k=7) Performance:\n",
            "R² Score: 0.6731\n",
            "Mean Absolute Error: 0.4440\n",
            "Mean Squared Error: 0.4283\n",
            "Root Mean Squared Error: 0.6545\n",
            "\n",
            "Training k-NN Regression model with k=9...\n",
            "\n",
            "k-NN (k=9) Performance:\n",
            "R² Score: 0.6760\n",
            "Mean Absolute Error: 0.4409\n",
            "Mean Squared Error: 0.4246\n",
            "Root Mean Squared Error: 0.6516\n",
            "\n",
            "Training k-NN Regression model with k=11...\n",
            "\n",
            "k-NN (k=11) Performance:\n",
            "R² Score: 0.6806\n",
            "Mean Absolute Error: 0.4384\n",
            "Mean Squared Error: 0.4186\n",
            "Root Mean Squared Error: 0.6470\n",
            "\n",
            "Model Performance Comparison:\n",
            "               Model        R²       MAE       MSE      RMSE\n",
            "0  Linear Regression  0.575788  0.533200  0.555892  0.745581\n",
            "1         k-NN (k=3)  0.643880  0.459920  0.466663  0.683128\n",
            "2         k-NN (k=5)  0.670010  0.446154  0.432422  0.657588\n",
            "3         k-NN (k=7)  0.673130  0.444039  0.428334  0.654472\n",
            "4         k-NN (k=9)  0.675961  0.440927  0.424623  0.651631\n",
            "5        k-NN (k=11)  0.680563  0.438432  0.418593  0.646988\n",
            "\n",
            "Analysis complete. All figures saved to current directory.\n",
            "The best k-NN model used k = 11\n",
            "\n",
            "Linear Regression Feature Importance:\n",
            "      Feature  Coefficient  Abs_Coefficient\n",
            "6    Latitude    -0.896929         0.896929\n",
            "7   Longitude    -0.869842         0.869842\n",
            "0      MedInc     0.854383         0.854383\n",
            "3   AveBedrms     0.339259         0.339259\n",
            "2    AveRooms    -0.294410         0.294410\n",
            "1    HouseAge     0.122546         0.122546\n",
            "5    AveOccup    -0.040829         0.040829\n",
            "4  Population    -0.002308         0.002308\n",
            "\n",
            "========== SUMMARY ==========\n",
            "Linear Regression strengths: Interpretable, fast, good for understanding feature relationships\n",
            "k-NN strengths: Captures non-linear patterns, no assumptions about data distribution\n",
            "Best performing model: k-NN (k=11) with R² = 0.6806\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# 1. Data Loading and Exploration\n",
        "print(\"Loading California Housing Dataset...\")\n",
        "housing = fetch_california_housing()\n",
        "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y = housing.target\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(\"\\nFeature names:\")\n",
        "for name in housing.feature_names:\n",
        "    print(f\"- {name}\")\n",
        "print(f\"\\nTarget variable: {housing.target_names[0]}\")\n",
        "\n",
        "# Display basic statistics\n",
        "print(\"\\nBasic statistics of features:\")\n",
        "print(X.describe())\n",
        "\n",
        "# 2. Data Visualization\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# Correlation matrix\n",
        "plt.subplot(2, 2, 1)\n",
        "correlation_matrix = np.corrcoef(X.values, y.reshape(-1, 1), rowvar=False)\n",
        "feature_names_with_target = housing.feature_names + ['MedHouseValue']\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', \n",
        "            xticklabels=feature_names_with_target, \n",
        "            yticklabels=feature_names_with_target)\n",
        "plt.title('Correlation Matrix')\n",
        "\n",
        "# Scatter plot of MedInc vs. House Value\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.scatter(X['MedInc'], y, alpha=0.5)\n",
        "plt.xlabel('Median Income')\n",
        "plt.ylabel('Median House Value')\n",
        "plt.title('Income vs. House Value')\n",
        "\n",
        "# Distribution of target variable\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.hist(y, bins=50)\n",
        "plt.xlabel('Median House Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of House Values')\n",
        "\n",
        "# Geographical distribution (Latitude vs. Longitude colored by price)\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.scatter(X['Longitude'], X['Latitude'], c=y, cmap='viridis', \n",
        "            alpha=0.5, s=10)\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "plt.title('House Prices across California')\n",
        "plt.colorbar(label='Median House Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('california_housing_eda.png')\n",
        "plt.close()\n",
        "\n",
        "# 3. Data Preparation\n",
        "# Split into training and testing sets (80% / 20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Model Training and Evaluation\n",
        "# Function to evaluate model performance\n",
        "def evaluate_model(model_name, y_true, y_pred):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    \n",
        "    print(f\"\\n{model_name} Performance:\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "    print(f\"Mean Absolute Error: {mae:.4f}\")\n",
        "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "    print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'R²': r2,\n",
        "        'MAE': mae,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse\n",
        "    }\n",
        "\n",
        "# 4.1 Linear Regression\n",
        "print(\"\\nTraining Linear Regression model...\")\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "lr_pred = lr_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate Linear Regression\n",
        "lr_metrics = evaluate_model(\"Linear Regression\", y_test, lr_pred)\n",
        "\n",
        "# 4.2 k-NN Regression with different k values\n",
        "k_values = [3, 5, 7, 9, 11]\n",
        "knn_metrics_list = []\n",
        "\n",
        "for k in k_values:\n",
        "    print(f\"\\nTraining k-NN Regression model with k={k}...\")\n",
        "    knn_model = KNeighborsRegressor(n_neighbors=k)\n",
        "    knn_model.fit(X_train_scaled, y_train)\n",
        "    knn_pred = knn_model.predict(X_test_scaled)\n",
        "    \n",
        "    # Evaluate k-NN\n",
        "    knn_metrics = evaluate_model(f\"k-NN (k={k})\", y_test, knn_pred)\n",
        "    knn_metrics_list.append(knn_metrics)\n",
        "\n",
        "# 5. Performance Comparison\n",
        "# Combine all metrics for comparison\n",
        "all_metrics = [lr_metrics] + knn_metrics_list\n",
        "metrics_df = pd.DataFrame(all_metrics)\n",
        "print(\"\\nModel Performance Comparison:\")\n",
        "print(metrics_df)\n",
        "\n",
        "# Visualize performance metrics\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# R² comparison\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.bar(metrics_df['Model'], metrics_df['R²'])\n",
        "plt.title('R² Score Comparison')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylim(0, 1)  # R² is typically between 0 and 1\n",
        "\n",
        "# MAE comparison\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.bar(metrics_df['Model'], metrics_df['MAE'])\n",
        "plt.title('Mean Absolute Error Comparison')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# MSE comparison\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.bar(metrics_df['Model'], metrics_df['MSE'])\n",
        "plt.title('Mean Squared Error Comparison')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# RMSE comparison\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.bar(metrics_df['Model'], metrics_df['RMSE'])\n",
        "plt.title('Root Mean Squared Error Comparison')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_comparison.png')\n",
        "plt.close()\n",
        "\n",
        "# Analyze feature importance for Linear Regression\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': housing.feature_names,\n",
        "    'Coefficient': lr_model.coef_\n",
        "})\n",
        "feature_importance['Abs_Coefficient'] = abs(feature_importance['Coefficient'])\n",
        "feature_importance = feature_importance.sort_values('Abs_Coefficient', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance['Feature'], feature_importance['Coefficient'])\n",
        "plt.title('Linear Regression Coefficients')\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.savefig('feature_importance.png')\n",
        "plt.close()\n",
        "\n",
        "# 6. Prediction Visualization\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Linear Regression predictions vs actual\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(y_test, lr_pred, alpha=0.5)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Linear Regression: Predicted vs Actual')\n",
        "\n",
        "# Best k-NN predictions vs actual\n",
        "best_knn_idx = metrics_df['R²'][1:].idxmax()\n",
        "best_knn_model = metrics_df['Model'][best_knn_idx]\n",
        "best_k = int(best_knn_model.split('=')[1].strip(')'))\n",
        "\n",
        "knn_model = KNeighborsRegressor(n_neighbors=best_k)\n",
        "knn_model.fit(X_train_scaled, y_train)\n",
        "knn_pred = knn_model.predict(X_test_scaled)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(y_test, knn_pred, alpha=0.5)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title(f'k-NN (k={best_k}): Predicted vs Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('prediction_comparison.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nAnalysis complete. All figures saved to current directory.\")\n",
        "print(\"The best k-NN model used k =\", best_k)\n",
        "print(\"\\nLinear Regression Feature Importance:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Summary of findings\n",
        "print(\"\\n========== SUMMARY ==========\")\n",
        "print(\"Linear Regression strengths: Interpretable, fast, good for understanding feature relationships\")\n",
        "print(\"k-NN strengths: Captures non-linear patterns, no assumptions about data distribution\")\n",
        "print(f\"Best performing model: {metrics_df.iloc[metrics_df['R²'].idxmax()]['Model']} with R² = {metrics_df['R²'].max():.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
